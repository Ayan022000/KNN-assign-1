{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfd13db",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "A1. The K-Nearest Neighbors (KNN) algorithm is a simple and popular supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying data distribution. KNN works by finding the 'K' nearest data points (neighbors) to a given query point in the feature space and using their labels (for classification) or average values (for regression) to make predictions for the query point.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "A2. The selection of the value of 'K' in KNN is crucial, as it can significantly impact the performance of the algorithm. A small 'K' can be sensitive to noise, while a large 'K' can smooth out decision boundaries. To choose the appropriate 'K', one can use techniques like cross-validation, where different values of 'K' are tested on the training data, and their performance is evaluated using metrics such as accuracy or mean squared error. The 'K' that provides the best performance on the validation set is then selected.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "A3. The difference lies in the type of task they are used for:\n",
    "\n",
    "KNN Classifier: This is used for classification tasks where the goal is to assign a class label to a given input data point. It predicts the class based on the majority class of the 'K' nearest neighbors.\n",
    "KNN Regressor: This is used for regression tasks where the goal is to predict a continuous numerical value (e.g., price or temperature). It predicts the output value based on the average or weighted average of the 'K' nearest neighbors' output values.\n",
    "\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "A4. The performance of the KNN algorithm is typically measured using various evaluation metrics, depending on the task (classification or regression):\n",
    "\n",
    "For classification tasks, common metrics include accuracy, precision, recall, F1-score, and area under the ROC curve (AUC).\n",
    "For regression tasks, metrics like mean squared error (MSE), mean absolute error (MAE), and R-squared (coefficient of determination) are commonly used.\n",
    "\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "A5. The curse of dimensionality in KNN refers to the deterioration of algorithm performance as the number of features (dimensions) increases. In high-dimensional spaces, data points tend to become more sparse, making it harder to find meaningful neighbors. This can lead to increased computational complexity and reduced accuracy. As the number of dimensions grows, the amount of data required to maintain the same level of performance grows exponentially.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "A6. Handling missing values in KNN is important since the algorithm relies on calculating distances between data points. There are various methods to handle missing values in KNN:\n",
    "\n",
    "Removing rows with missing values: This can be done if the dataset has a sufficient number of samples and the missing values are minimal.\n",
    "Imputing missing values: Missing values can be filled in using techniques like mean, median, or mode imputation based on the values of neighboring data points.\n",
    "Using algorithms that support missing values: Some KNN implementations have provisions to handle missing values directly during the distance calculation.\n",
    "\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "A7. The choice between KNN classifier and regressor depends on the nature of the problem:\n",
    "\n",
    "KNN Classifier: It is suitable for problems where the output is discrete and categorical, such as text classification, image recognition, or predicting the type of disease based on symptoms.\n",
    "KNN Regressor: It is appropriate for problems involving continuous numerical output, like predicting housing prices, temperature, or stock prices.\n",
    "\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "A8. Strengths of KNN:\n",
    "\n",
    "Simple to understand and implement.\n",
    "Effective for nonlinear relationships in data.\n",
    "Robust to noisy data.\n",
    "Weaknesses of KNN:\n",
    "\n",
    "Computationally expensive, especially for large datasets.\n",
    "Sensitive to the choice of 'K' and distance metric.\n",
    "Performs poorly on high-dimensional data (curse of dimensionality).\n",
    "Addressing weaknesses:\n",
    "\n",
    "Using efficient data structures like KD-trees can speed up nearest neighbor search.\n",
    "Feature selection or dimensionality reduction techniques can be applied to reduce dimensionality.\n",
    "Optimal 'K' and distance metric can be determined through cross-validation.\n",
    "\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "A9. Both Euclidean distance and Manhattan distance are distance metrics used in KNN to measure the similarity between data points.\n",
    "\n",
    "Euclidean distance: It is the straight-line distance between two points in the feature space. In a 2-dimensional space, it is the length of the hypotenuse of the right triangle formed by the two points. Mathematically, for two points (x1, y1) and (x2, y2), the Euclidean distance is given by âˆš((x2 - x1)^2 + (y2 - y1)^2).\n",
    "\n",
    "Manhattan distance: It is also known as the city-block distance or L1 norm distance. Instead of the straight-line distance, it measures the distance as the sum of the absolute differences between the coordinates of two points. Mathematically, for two points (x1, y1) and (x2, y2), the Manhattan distance is given by |x2 - x1| + |y2 - y1|.\n",
    "\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "A10. Feature scaling is essential in KNN because the algorithm relies heavily on the distance between data points. If the features have different scales, those with larger magnitudes can dominate the distance calculations, leading to biased results. Feature scaling ensures that all features contribute equally to the distance calculations.\n",
    "\n",
    "Two common methods of feature scaling are:\n",
    "\n",
    "Min-Max scaling (Normalization): Scales the features to a specified range, usually [0, 1].\n",
    "Standardization (Z-score scaling): Transforms the features to have a mean of 0 and a standard deviation of 1.\n",
    "Applying feature scaling before using KNN helps to improve the performance and convergence of the algorithm, especially when the features have different units or ranges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
